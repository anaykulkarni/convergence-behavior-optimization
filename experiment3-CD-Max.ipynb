{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        wine = load_wine(as_frame=True)\n",
    "        dataset = pd.concat([wine.data, wine.target], axis=1)\n",
    "        # print(f'Original Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        dataset = dataset[dataset['target'] < 2]\n",
    "        # print(f'Updated Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        self.X = torch.tensor(dataset.iloc[:, :13].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(dataset.iloc[:, 13].values, dtype=torch.float32)\n",
    "\n",
    "        # normalize X\n",
    "        self.X = (self.X - self.X.mean(dim=0)) / self.X.std(dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.int64)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w, X, b):\n",
    "    return F.sigmoid((X @ w) + b)\n",
    "\n",
    "def loss_fn(proba, truth):\n",
    "    log_p = torch.log(proba)\n",
    "    log_q = torch.log(1 - proba)\n",
    "    return -1 * torch.mean(truth * log_p + (1-truth) * log_q)\n",
    "\n",
    "def weight_gradients(X, y_proba, y_truth):\n",
    "    grads = (y_proba - y_truth).unsqueeze(-1) * X\n",
    "    return torch.mean(grads, dim=0) # mean over batch\n",
    "\n",
    "def bias_gradient(y_proba, y_truth):\n",
    "    grads = (y_proba - y_truth)\n",
    "    return torch.mean(grads, dim=0) # mean over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, w, b, lr):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    train_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        y_proba = forward(w, X, b)\n",
    "        loss = loss_fn(y_proba, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += ((y_proba > 0.5).int() == y).float().sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        grads_w = weight_gradients(X, y_proba, y)\n",
    "        grad_b = bias_gradient(y_proba, y)\n",
    "\n",
    "        # Update bias\n",
    "        b = b - lr * (grad_b)\n",
    "        \n",
    "        # choose coordinate\n",
    "        i = torch.argmax(grads_w)\n",
    "        w[i] = w[i] - lr * grads_w[i]\n",
    "    \n",
    "    average_train_loss = train_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader, w, b):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    eval_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        y_proba = forward(w, X, b)\n",
    "        loss = loss_fn(y_proba, y)\n",
    "        eval_loss += loss.item()\n",
    "        correct += ((y_proba > 0.5).int() == y).float().sum().item()\n",
    "\n",
    "    average_eval_loss = eval_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(batch_size, epochs, lr):\n",
    "    dataset = WineDataset()\n",
    "\n",
    "    # Define sizes (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    w = torch.randn(13) * 0.1\n",
    "    b = torch.randn(1)\n",
    "\n",
    "    train_losses = []\n",
    "    for e in range(1, epochs+1):\n",
    "        train_loss, train_accuracy = train_model(train_loader, w, b, lr)\n",
    "        train_losses.append(train_loss)\n",
    "        if e % 25 == 0:\n",
    "            print(f'Training Epoch {e}/{epochs}: Train Loss: {train_loss}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "    test_loss, test_accuracy = eval_model(test_loader, w, b)\n",
    "    print(f'Test evaluation: Loss: {test_loss}, Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/300: Train Loss: 0.6211251957075936, Accuracy: 0.5481\n",
      "Training Epoch 50/300: Train Loss: 0.49870396511895315, Accuracy: 0.6538\n",
      "Training Epoch 75/300: Train Loss: 0.41698384497846874, Accuracy: 0.7596\n",
      "Training Epoch 100/300: Train Loss: 0.33064909279346466, Accuracy: 0.8077\n",
      "Training Epoch 125/300: Train Loss: 0.2954728113753455, Accuracy: 0.8462\n",
      "Training Epoch 150/300: Train Loss: 0.24743767082691193, Accuracy: 0.8654\n",
      "Training Epoch 175/300: Train Loss: 0.24325019334043776, Accuracy: 0.8750\n",
      "Training Epoch 200/300: Train Loss: 0.20591883999960764, Accuracy: 0.8942\n",
      "Training Epoch 225/300: Train Loss: 0.1928394255893571, Accuracy: 0.9135\n",
      "Training Epoch 250/300: Train Loss: 0.1983348514352526, Accuracy: 0.9327\n",
      "Training Epoch 275/300: Train Loss: 0.18885755113192967, Accuracy: 0.9327\n",
      "Training Epoch 300/300: Train Loss: 0.18039724550076894, Accuracy: 0.9519\n",
      "Test evaluation: Loss: 0.13405419141054153, Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "losses = experiment(16, 300, 0.01)\n",
    "with open(\"losses-CD-Max.json\", \"w\") as file:\n",
    "    json.dump(losses, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
