{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        wine = load_wine(as_frame=True)\n",
    "        dataset = pd.concat([wine.data, wine.target], axis=1)\n",
    "        # print(f'Original Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        dataset = dataset[dataset['target'] < 2]\n",
    "        # print(f'Updated Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        self.X = torch.tensor(dataset.iloc[:, :13].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(dataset.iloc[:, 13].values, dtype=torch.float32)\n",
    "\n",
    "        # normalize X\n",
    "        self.X = (self.X - self.X.mean(dim=0)) / self.X.std(dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.int64)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w, X, b):\n",
    "    return F.sigmoid((X @ w) + b)\n",
    "\n",
    "def loss_fn(proba, truth):\n",
    "    log_p = torch.log(proba)\n",
    "    log_q = torch.log(1 - proba)\n",
    "    return -1 * torch.mean(truth * log_p + (1-truth) * log_q)\n",
    "\n",
    "def weight_gradients(X, y_proba, y_truth):\n",
    "    grads = (y_proba - y_truth).unsqueeze(-1) * X\n",
    "    return torch.mean(grads, dim=0) # mean over batch\n",
    "\n",
    "def bias_gradient(y_proba, y_truth):\n",
    "    grads = (y_proba - y_truth)\n",
    "    return torch.mean(grads, dim=0) # mean over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, w, b, lr, k):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    train_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        y_proba = forward(w, X, b)\n",
    "        loss = loss_fn(y_proba, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += ((y_proba > 0.5).int() == y).float().sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        grads_w = weight_gradients(X, y_proba, y)\n",
    "        grad_b = bias_gradient(y_proba, y)\n",
    "\n",
    "        # Update bias\n",
    "        b = b - lr * (grad_b)\n",
    "        \n",
    "        # Choose top-k coordinates\n",
    "        indices = torch.argsort(torch.abs(grads_w), descending=True)\n",
    "        update_idx = indices[:k]  # Select top-k coordinates\n",
    "\n",
    "        # Update the top-k coordinates\n",
    "        w[update_idx] = w[update_idx] - lr * grads_w[update_idx]\n",
    "\n",
    "        # Decay all other coordinates to zero\n",
    "        zero_idx = indices[k:]  # Indices of the bottom d-k coordinates\n",
    "        w[zero_idx] *= 0.9 \n",
    "    \n",
    "    average_train_loss = train_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader, w, b):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    eval_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        y_proba = forward(w, X, b)\n",
    "        loss = loss_fn(y_proba, y)\n",
    "        eval_loss += loss.item()\n",
    "        correct += ((y_proba > 0.5).int() == y).float().sum().item()\n",
    "\n",
    "    average_eval_loss = eval_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(batch_size, epochs, lr, k):\n",
    "    dataset = WineDataset()\n",
    "\n",
    "    # Define sizes (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    w = torch.zeros(13)\n",
    "    b = torch.zeros(1)\n",
    "\n",
    "    train_losses = []\n",
    "    for e in range(1, epochs+1):\n",
    "        train_loss, train_accuracy = train_model(train_loader, w, b, lr, k)\n",
    "        train_losses.append(train_loss)\n",
    "        if e % 25 == 0:\n",
    "            print(f'Training Epoch {e}/{epochs}: Train Loss: {train_loss}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "    test_loss, test_accuracy = eval_model(test_loader, w, b)\n",
    "    print(f'Test evaluation: Loss: {test_loss}, Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2(batch_size, epochs, lr, k):\n",
    "    dataset = WineDataset()\n",
    "\n",
    "    # Define sizes (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    w = torch.zeros(13)\n",
    "    b = torch.zeros(1)\n",
    "\n",
    "    data = []\n",
    "    for e in range(1, epochs+1):\n",
    "        train_loss, train_accuracy = train_model(train_loader, w, b, lr, k)\n",
    "        if e % 300 == 0:\n",
    "            data.extend([train_loss, train_accuracy])\n",
    "        \n",
    "    test_loss, test_accuracy = eval_model(test_loader, w, b)\n",
    "    data.extend([test_loss, test_accuracy])\n",
    "\n",
    "    # print(\"Params, \", w, b)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/300: Train Loss: 0.3034490815230778, Accuracy: 0.9904\n",
      "Training Epoch 50/300: Train Loss: 0.23803700506687164, Accuracy: 1.0000\n",
      "Training Epoch 75/300: Train Loss: 0.2339185561452593, Accuracy: 1.0000\n",
      "Training Epoch 100/300: Train Loss: 0.23800951029573167, Accuracy: 0.9904\n",
      "Training Epoch 125/300: Train Loss: 0.20268521351473673, Accuracy: 1.0000\n",
      "Training Epoch 150/300: Train Loss: 0.21066935786179133, Accuracy: 0.9904\n",
      "Training Epoch 175/300: Train Loss: 0.20849287935665675, Accuracy: 0.9904\n",
      "Training Epoch 200/300: Train Loss: 0.20396849513053894, Accuracy: 1.0000\n",
      "Training Epoch 225/300: Train Loss: 0.18715218773909978, Accuracy: 0.9904\n",
      "Training Epoch 250/300: Train Loss: 0.20030975554670608, Accuracy: 1.0000\n",
      "Training Epoch 275/300: Train Loss: 0.21640215814113617, Accuracy: 1.0000\n",
      "Training Epoch 300/300: Train Loss: 0.18903888123376028, Accuracy: 0.9904\n",
      "Test evaluation: Loss: 0.2127237468957901, Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "losses = experiment(16, 300, 0.01, 10)\n",
    "with open(\"losses-CD-KSparse.json\", \"w\") as file:\n",
    "    json.dump(losses, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_data = {}\n",
    "for k in range(1, 14):\n",
    "    execution_data[k] = []\n",
    "    for trial in range(20):\n",
    "        execution_data[k].append(experiment2(16, 300, 0.01, k))\n",
    "\n",
    "with open(\"losses-CD-Ksparse-Trials2.json\", \"w\") as file:\n",
    "    json.dump(execution_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
