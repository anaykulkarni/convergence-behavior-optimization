{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        wine = load_wine(as_frame=True)\n",
    "        dataset = pd.concat([wine.data, wine.target], axis=1)\n",
    "        print(f'Original Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        dataset = dataset[dataset['target'] < 2]\n",
    "        print(f'Updated Wine Dataset: Samples = {len(dataset)}, Labels = {dataset[\"target\"].unique()}, Features = {len(dataset.columns)-1}')\n",
    "        self.X = torch.tensor(dataset.iloc[:, :13].values)\n",
    "        self.y = torch.tensor(dataset.iloc[:, 13].values)\n",
    "\n",
    "        # normalize X\n",
    "        self.X = (self.X - self.X.mean(dim=0)) / self.X.std(dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.int64)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, feat_dim=13, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(feat_dim, output_dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).float().sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    average_train_loss = train_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader, model, loss_fn):\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, correct = 0.0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        eval_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).float().sum().item()\n",
    "\n",
    "    average_eval_loss = eval_loss/num_batches\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return average_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(batch_size, epochs, lr):\n",
    "    dataset = WineDataset()\n",
    "\n",
    "    # Define sizes (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LogisticRegressionModel(feat_dim=13, output_dim=2)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    for e in range(1, epochs+1):\n",
    "        train_loss, train_accuracy = train_model(train_loader, model, loss_fn, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        if e % 25 == 0:\n",
    "            print(f'Training Epoch {e}/{epochs}: Train Loss: {train_loss}, Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "    test_loss, test_accuracy = eval_model(test_loader, model, loss_fn)\n",
    "    print(f'Test evaluation: Loss: {test_loss}, Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Wine Dataset: Samples = 178, Labels = [0 1 2], Features = 13\n",
      "Updated Wine Dataset: Samples = 130, Labels = [0 1], Features = 13\n",
      "Training Epoch 25/300: Train Loss: 0.1829517654010228, Accuracy: 0.9423\n",
      "Training Epoch 50/300: Train Loss: 0.11985122306006295, Accuracy: 0.9808\n",
      "Training Epoch 75/300: Train Loss: 0.0911063626408577, Accuracy: 1.0000\n",
      "Training Epoch 100/300: Train Loss: 0.07798221388033458, Accuracy: 1.0000\n",
      "Training Epoch 125/300: Train Loss: 0.06328266326870237, Accuracy: 1.0000\n",
      "Training Epoch 150/300: Train Loss: 0.05940225374485765, Accuracy: 1.0000\n",
      "Training Epoch 175/300: Train Loss: 0.05318280043346541, Accuracy: 1.0000\n",
      "Training Epoch 200/300: Train Loss: 0.04422071283417089, Accuracy: 1.0000\n",
      "Training Epoch 225/300: Train Loss: 0.04329369296985013, Accuracy: 1.0000\n",
      "Training Epoch 250/300: Train Loss: 0.036922618480665345, Accuracy: 1.0000\n",
      "Training Epoch 275/300: Train Loss: 0.03617765421846083, Accuracy: 1.0000\n",
      "Training Epoch 300/300: Train Loss: 0.033970478789082596, Accuracy: 1.0000\n",
      "Test evaluation: Loss: 0.05067484453320503, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "losses = experiment(16, 300, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses-SGD.json\", \"w\") as file:\n",
    "    json.dump(losses, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
